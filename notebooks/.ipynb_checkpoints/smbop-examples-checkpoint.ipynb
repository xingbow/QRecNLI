{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93067442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/xingbo/chi2022/seqNLI/backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "ROOT = os.path.join(os.path.dirname(os.getcwd()), 'backend')\n",
    "print(ROOT)\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.append(ROOT)\n",
    "    \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5aaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT, \"app/data/model/SmBop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15dc2cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xingbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/xingbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pathlib\n",
    "import gdown\n",
    "import argparse\n",
    "import torch\n",
    "from allennlp.models.archival import Archive, load_archive, archive_model\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from smbop.modules.relation_transformer import *\n",
    "from allennlp.common import Params\n",
    "from smbop.models.smbop import SmbopParser\n",
    "from smbop.modules.lxmert import LxmertCrossAttentionLayer\n",
    "from smbop.dataset_readers.spider import SmbopSpiderDatasetReader\n",
    "import itertools\n",
    "import smbop.utils.node_util as node_util\n",
    "import numpy as np\n",
    "from allennlp.models import Model\n",
    "from allennlp.common.params import *\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "import tqdm\n",
    "from allennlp.predictors import Predictor\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40194e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SmBop', 'smbop.tar.gz']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(os.getcwd(), \"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3436294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load_trees\n",
      "before connecting\n",
      "before load_trees\n",
      "before connecting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/grappa_large_jnt were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias_col', 'lm_head.dense_col.weight', 'lm_head.layer_norm.bias', 'lm_head.dense_col.bias', 'q_tab_dense.weight', 'lm_head.layer_norm_col.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm_col.bias', 'lm_head.decoder_col.weight', 'lm_head.layer_norm.weight', 'q_tab_dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amp': True, 'base_dim': 32, 'batch_size': 20, 'beam_encoder_num_layers': 1, 'cntx_beam': False, 'cntx_rep': False, 'cntx_reranker': True, 'debug': False, 'disentangle_cntx': True, 'embedding_dim': 256, 'grad_acum': 4, 'grad_clip': None, 'grad_norm': None, 'is_oracle': False, 'lin_after_cntx': False, 'lm_lr': 3e-06, 'load_less': False, 'lr': 0.000186, 'max_steps': 60000, 'num_heads': 8, 'power': 0.5, 'rat_dropout': 0.2, 'rat_layers': 8, 'should_rerank': False, 'temperature': 1, 'tfixup': False, 'tiny_dataset': False, 'train_as_dev': False, 'tree_rep_transformer_num_layers': 1, 'uniquify': False, 'use_bce': False, 'use_treelstm': False, 'utt_aug': True, 'value_pred': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT( * ) FROM schedule WHERE schedule.price<10'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.isdir(\"./cache\"):\n",
    "    os.mkdir(\"cache\")\n",
    "overrides = {\n",
    "    \"dataset_reader\": {\n",
    "        \"tables_file\": \"../../dataset/spider/tables.json\",\n",
    "        \"dataset_path\": \"../../dataset/spider/database\",\n",
    "    },\n",
    "    \"validation_dataset_reader\": {\n",
    "    \"tables_file\": \"../../dataset/spider/tables.json\",\n",
    "    \"dataset_path\": \"../../dataset/spider/database\",\n",
    "    }\n",
    "}\n",
    "predictor = Predictor.from_path(\n",
    "    \"../smbop.tar.gz\", cuda_device=0, overrides=overrides\n",
    ")\n",
    "instance_0 = predictor._dataset_reader.text_to_instance(\n",
    "    utterance=\"asds\", db_id=\"aircraft\"\n",
    ")\n",
    "predictor._dataset_reader.apply_token_indexers(instance_0)\n",
    "def inference(question,db_id):\n",
    "  instance = predictor._dataset_reader.text_to_instance(\n",
    "      utterance=question, db_id=db_id,\n",
    "  )\n",
    "  predictor._dataset_reader.apply_token_indexers(instance)\n",
    "  with torch.cuda.amp.autocast(enabled=True):\n",
    "      out = predictor._model.forward_on_instances(\n",
    "          [instance, instance_0]\n",
    "      )\n",
    "      return out[0][\"sql_list\"]\n",
    "inference(\"How many films cost below 10 dollars?\",\"cinema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0814f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t db_id: MetadataField (print field.metadata to see specific information). \n",
      " \t relation: TensorField with shape: torch.Size([46, 46]) and dtype: torch.int32. \n",
      " \t entities: MetadataField (print field.metadata to see specific information). \n",
      " \t orig_entities: MetadataField (print field.metadata to see specific information). \n",
      " \t leaf_hash: TensorField with shape: torch.Size([34]) and dtype: torch.int64. \n",
      " \t leaf_types: TensorField with shape: torch.Size([34]) and dtype: torch.int32. \n",
      " \t span_hash: TensorField with shape: torch.Size([12, 12]) and dtype: torch.int64. \n",
      " \t lengths: TensorField with shape: torch.Size([2, 2]) and dtype: torch.int32. \n",
      " \t offsets: TensorField with shape: torch.Size([46, 2]) and dtype: torch.int32. \n",
      " \t enc: TextField of length 179 with text: \n",
      " \t\t[<s>, Which, Ġfilms, Ġcost, Ġmore, Ġthan, Ġ50, Ġdollars, Ġor, Ġless, Ġthan, Ġ10, ?, </s>, 1, 2, 3,\n",
      "\t\t4, 5, yes, no, y, t, f, m, n, null, *, c, inem, a, (, c, inem, a, ., capacity, :, number, ,, c,\n",
      "\t\tinem, a, ., c, inem, a, id, :, number, ,, c, inem, a, ., location, :, text, ,, c, inem, a, ., name,\n",
      "\t\t:, text, ,, c, inem, a, ., open, ning, year, :, number, ), Ċ, film, (, film, ., directed, by, :,\n",
      "\t\ttext, ,, film, ., film, id, :, number, ,, film, ., number, in, season, :, number, ,, film, .,\n",
      "\t\toriginal, air, date, :, text, ,, film, ., production, code, :, text, ,, film, ., rank, in, series,\n",
      "\t\t:, number, ,, film, ., title, :, text, ), Ċ, sche, dule, (, sche, dule, ., c, inem, a, id, :,\n",
      "\t\tnumber, ,, sche, dule, ., date, :, text, ,, sche, dule, ., film, id, :, number, ,, sche, dule, .,\n",
      "\t\tprice, :, number, ,, sche, dule, ., show, times, per, day, :, number, ), Ċ, </s>]\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Which films cost more than 50 dollars or less than 10?\"\n",
    "db_id = \"cinema\"\n",
    "instance = predictor._dataset_reader.text_to_instance(\n",
    "      utterance=question, db_id=db_id,\n",
    ")\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f690c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_abc_impl', 'add_field', 'as_tensor_dict', 'count_vocab_items', 'duplicate', 'fields', 'get', 'get_padding_lengths', 'human_readable_dict', 'index_fields', 'indexed', 'items', 'keys', 'values']\n",
      "{'db_id': <allennlp.data.fields.metadata_field.MetadataField object at 0x7fdaeeefa250>, 'relation': <allennlp.data.fields.tensor_field.TensorField object at 0x7fdaeeefcb90>, 'entities': <allennlp.data.fields.metadata_field.MetadataField object at 0x7fdab4240250>, 'orig_entities': <allennlp.data.fields.metadata_field.MetadataField object at 0x7fda1fdae590>, 'leaf_hash': <allennlp.data.fields.tensor_field.TensorField object at 0x7fda1fdae5d0>, 'leaf_types': <allennlp.data.fields.tensor_field.TensorField object at 0x7fda1fdae610>, 'span_hash': <allennlp.data.fields.tensor_field.TensorField object at 0x7fda1fdae690>, 'lengths': <allennlp.data.fields.tensor_field.TensorField object at 0x7fda1fdae650>, 'offsets': <allennlp.data.fields.tensor_field.TensorField object at 0x7fda1fdae6d0>, 'enc': <allennlp.data.fields.text_field.TextField object at 0x7fdab42843c0>}\n",
      "entities (length: 34): ['1', '2', '3', '4', '5', 'yes', 'no', 'y', 't', 'f', 'm', 'n', 'null', '*', 'cinema', 'cinema.capacity', 'cinema.cinema_id', 'cinema.location', 'cinema.name', 'cinema.openning_year', 'film', 'film.directed_by', 'film.film_id', 'film.number_in_season', 'film.original_air_date', 'film.production_code', 'film.rank_in_series', 'film.title', 'schedule', 'schedule.cinema_id', 'schedule.date', 'schedule.film_id', 'schedule.price', 'schedule.show_times_per_day']\n",
      "TextField of length 179 with text: \n",
      " \t\t[<s>, Which, Ġfilms, Ġcost, Ġmore, Ġthan, Ġ50, Ġdollars, Ġor, Ġless, Ġthan, Ġ10, ?, </s>, 1, 2, 3,\n",
      "\t\t4, 5, yes, no, y, t, f, m, n, null, *, c, inem, a, (, c, inem, a, ., capacity, :, number, ,, c,\n",
      "\t\tinem, a, ., c, inem, a, id, :, number, ,, c, inem, a, ., location, :, text, ,, c, inem, a, ., name,\n",
      "\t\t:, text, ,, c, inem, a, ., open, ning, year, :, number, ), Ċ, film, (, film, ., directed, by, :,\n",
      "\t\ttext, ,, film, ., film, id, :, number, ,, film, ., number, in, season, :, number, ,, film, .,\n",
      "\t\toriginal, air, date, :, text, ,, film, ., production, code, :, text, ,, film, ., rank, in, series,\n",
      "\t\t:, number, ,, film, ., title, :, text, ), Ċ, sche, dule, (, sche, dule, ., c, inem, a, id, :,\n",
      "\t\tnumber, ,, sche, dule, ., date, :, text, ,, sche, dule, ., film, id, :, number, ,, sche, dule, .,\n",
      "\t\tprice, :, number, ,, sche, dule, ., show, times, per, day, :, number, ), Ċ, </s>]\n",
      "\n",
      "leaf types: [36 36 36 36 36 36 36 36 36 36 36 36 36 36 35 36 36 36 36 36 35 36 36 36\n",
      " 36 36 36 36 35 36 36 36 36 36]\n"
     ]
    }
   ],
   "source": [
    "print(dir(instance))\n",
    "print(instance.fields)\n",
    "print(\"entities (length: {}): {}\".format(len(instance.fields[\"entities\"].metadata), instance.fields[\"entities\"].metadata))\n",
    "print(instance.fields[\"enc\"])\n",
    "print(\"leaf types: {}\".format(instance.fields[\"leaf_types\"].array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38917dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '_ACTIONS',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_action_dim',\n",
       " '_apply',\n",
       " '_augment_with_utterance',\n",
       " '_backward_hooks',\n",
       " '_batch_size',\n",
       " '_bce_loss',\n",
       " '_beam_encoder',\n",
       " '_beam_size',\n",
       " '_beam_summarizer',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compute_validation_outputs',\n",
       " '_create_action_dicts',\n",
       " '_create_beam_rep',\n",
       " '_create_type_tensor',\n",
       " '_decoder_timesteps',\n",
       " '_device',\n",
       " '_emb_to_action_dim',\n",
       " '_encode_utt_schema',\n",
       " '_evaluate_func',\n",
       " '_experiment_name',\n",
       " '_final_beam_acc',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_frontier_size',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_get_prediction_device',\n",
       " '_is_full_backward_hook',\n",
       " '_leafs_acc',\n",
       " '_load',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_for_unseparable_batches',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_misc_params',\n",
       " '_modules',\n",
       " '_n_schema_leafs',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_num_values',\n",
       " '_op_count',\n",
       " '_op_names',\n",
       " '_parameters',\n",
       " '_pooler',\n",
       " '_question_embedder',\n",
       " '_rank_beam',\n",
       " '_rank_schema',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_registry',\n",
       " '_regularizer',\n",
       " '_replicate_for_data_parallel',\n",
       " '_reranker_acc',\n",
       " '_rule_tensor',\n",
       " '_rule_tensor_flat',\n",
       " '_save_to_state_dict',\n",
       " '_schema_encoder',\n",
       " '_slow_forward',\n",
       " '_softmax',\n",
       " '_span_score_func',\n",
       " '_spider_acc',\n",
       " '_state_dict_hooks',\n",
       " '_term_ids',\n",
       " '_term_tensor',\n",
       " '_tree_rep_transformer',\n",
       " '_type_dict',\n",
       " '_unary_frontier_embedder',\n",
       " '_utterance_augmenter',\n",
       " '_version',\n",
       " '_warn_for_unseparable_batches',\n",
       " 'activation_func',\n",
       " 'add_module',\n",
       " 'add_residual_beam',\n",
       " 'add_residual_reranker',\n",
       " 'after_add',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'binary_op_count',\n",
       " 'buffers',\n",
       " 'by_name',\n",
       " 'children',\n",
       " 'cntx_beam',\n",
       " 'cntx_rep',\n",
       " 'cntx_reranker',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'd_frontier',\n",
       " 'debug',\n",
       " 'default_implementation',\n",
       " 'default_predictor',\n",
       " 'disentangle_cntx',\n",
       " 'double',\n",
       " 'dropout_prob',\n",
       " 'dump_patches',\n",
       " 'emb_q',\n",
       " 'eval',\n",
       " 'extend_embedder_vocab',\n",
       " 'extra_repr',\n",
       " 'flag_move_to_gpu',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'forward_on_instance',\n",
       " 'forward_on_instances',\n",
       " 'from_archive',\n",
       " 'from_params',\n",
       " 'get_metrics',\n",
       " 'get_parameters_for_histogram_logging',\n",
       " 'get_parameters_for_histogram_tensorboard_logging',\n",
       " 'get_regularization_penalty',\n",
       " 'half',\n",
       " 'hash_frontier',\n",
       " 'hasher',\n",
       " 'is_oracle',\n",
       " 'keep_id',\n",
       " 'left_emb',\n",
       " 'lin_after_cntx',\n",
       " 'list_available',\n",
       " 'load',\n",
       " 'load_state_dict',\n",
       " 'make_output_human_readable',\n",
       " 'modules',\n",
       " 'move_to_gpu',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'oldlstm',\n",
       " 'only_last_rerank',\n",
       " 'op_count',\n",
       " 'op_linear',\n",
       " 'parameters',\n",
       " 'pre_op_linear',\n",
       " 'q_emb_dim',\n",
       " 'ranking_ratio',\n",
       " 'register',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'resolve_class_name',\n",
       " 'reuse_cntx_reranker',\n",
       " 'right_emb',\n",
       " 'score_frontier',\n",
       " 'score_spans',\n",
       " 'serialization_dir',\n",
       " 'set_flags',\n",
       " 'set_hash',\n",
       " 'share_memory',\n",
       " 'should_rerank',\n",
       " 'state_dict',\n",
       " 'summrize_vec',\n",
       " 'temperature',\n",
       " 'to',\n",
       " 'tokenizer',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'type_embedding',\n",
       " 'typecheck_frontier',\n",
       " 'unary_op_count',\n",
       " 'unique_reranker',\n",
       " 'uniquify',\n",
       " 'use_bce',\n",
       " 'use_treelstm',\n",
       " 'utt_aug',\n",
       " 'value_pred',\n",
       " 'vocab',\n",
       " 'xent',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(predictor._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b46b70a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_op_names: ['eq', 'like', 'nlike', 'add', 'sub', 'nin', 'lte', 'lt', 'neq', 'in', 'gte', 'gt', 'And', 'Or', 'except', 'union', 'intersect', 'Product', 'Val_list', 'Orderby_desc', 'Orderby_asc', 'Project', 'Selection', 'Limit', 'Groupby', 'keep', 'min', 'count', 'max', 'avg', 'sum', 'Subquery', 'distinct', 'literal', 'nan', 'Table', 'Value']\n",
      "_op_count: 37\n",
      "output keys: \u001b[31mdict_keys(['leaf_beam_hash', 'hash_gold_levelorder', 'inf_time', 'total_time', 'beam_scores', 'beam_encoding', 'beam_hash', 'gold_hash', 'reranker_acc', 'spider_acc', 'sql_list', 'tree_list', 'final_beam_acc', 'leaf_acc'])\u001b[0m\n",
      "hash_gold_levelorder: None\n",
      "inf_time: 0.06479501724243164\n",
      "total_time: 0.11849021911621094\n",
      "gold_hash: None\n",
      "reranker_acc: 0\n",
      "spider_acc: 0\n",
      "sql_list: SELECT film.title FROM schedule JOIN film ON schedule.film_id = film.film_id WHERE schedule.price > 50 OR schedule.price<10\n",
      "tree_list: Node('/keep')\n",
      "leaf_acc: None\n",
      "beam length:  10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(predictor._model.named_children)\n",
    "# !pip install termcolor\n",
    "from termcolor import colored\n",
    "print(\"_op_names: {}\".format(predictor._model._op_names))\n",
    "print(\"_op_count: {}\".format(predictor._model._op_count))\n",
    "predictor._dataset_reader.apply_token_indexers(instance)\n",
    "out = predictor._model.forward_on_instance(instance)\n",
    "\n",
    "print(\"output keys: {}\".format(colored(out.keys(), \"red\")))\n",
    "\n",
    "for okey in out.keys():\n",
    "  if \"beam\" not in okey:\n",
    "    print(\"{}: {}\".format(okey, out[okey]))\n",
    "\n",
    "print(\"beam length: \", len(out[\"beam_encoding\"]))\n",
    "# print(dir(out[\"beam_encoding\"][0]))\n",
    "# print(\"enc: {}\".format(out[\"beam_encoding\"][0].enc))\n",
    "# print(out[\"beam_encoding\"][0].final_leaf_indices)\n",
    "# print(len(out[\"beam_encoding\"][0].entities[0]), out[\"beam_encoding\"][0].entities)\n",
    "# print( [out[\"beam_encoding\"][0].entities[0][i] for i in out[\"beam_encoding\"][0].final_leaf_indices.cpu().numpy()[0]])\n",
    "# print(out[\"beam_encoding\"][0].curr_type)\n",
    "\n",
    "# 'enc', 'entities', 'final_leaf_indices', 'span_end_indices', 'span_start_indices'\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87e16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film.title: ['Value', 'keep', 'keep', 'keep', 'keep', 'Project', 'keep', 'keep', 'keep']\n",
      "schedule.film_id: ['Value', 'eq', 'keep', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "film.film_id: ['Value', 'eq', 'keep', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "schedule.price: ['Value', 'gt', 'Or', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "50: ['Value', 'gt', 'Or', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "schedule.price: ['Value', 'lt', 'Or', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "10: ['Value', 'lt', 'Or', 'And', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "schedule: ['Table', 'Product', 'keep', 'keep', 'Selection', 'Project', 'keep', 'keep', 'keep']\n",
      "film: ['Table', 'Product', 'keep', 'keep', 'Selection', 'Project', 'keep', 'keep', 'keep']\n"
     ]
    }
   ],
   "source": [
    "for leaf in out[\"tree_list\"].leaves:\n",
    "  leaf_path = []\n",
    "  leaf_val = leaf.val\n",
    "  for node in leaf.iter_path_reverse():\n",
    "    if node.is_root:\n",
    "      break\n",
    "    else:\n",
    "      leaf_path.append(node.name)\n",
    "  print(\"{}: {}\".format(leaf_val, leaf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdece17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node('/keep')\n",
      "└── Node('/keep/keep')\n",
      "    └── Node('/keep/keep/keep')\n",
      "        └── Node('/keep/keep/keep/keep')\n",
      "            └── Node('/keep/keep/keep/keep/Project')\n",
      "                ├── Node('/keep/keep/keep/keep/Project/keep')\n",
      "                │   └── Node('/keep/keep/keep/keep/Project/keep/keep')\n",
      "                │       └── Node('/keep/keep/keep/keep/Project/keep/keep/keep')\n",
      "                │           └── Node('/keep/keep/keep/keep/Project/keep/keep/keep/keep')\n",
      "                │               └── Node('/keep/keep/keep/keep/Project/keep/keep/keep/keep/Value', val='film.title')\n",
      "                └── Node('/keep/keep/keep/keep/Project/Selection')\n",
      "                    ├── Node('/keep/keep/keep/keep/Project/Selection/And')\n",
      "                    │   ├── Node('/keep/keep/keep/keep/Project/Selection/And/keep')\n",
      "                    │   │   └── Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq')\n",
      "                    │   │       ├── Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='schedule.film_id')\n",
      "                    │   │       └── Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='film.film_id')\n",
      "                    │   └── Node('/keep/keep/keep/keep/Project/Selection/And/Or')\n",
      "                    │       ├── Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt')\n",
      "                    │       │   ├── Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='schedule.price')\n",
      "                    │       │   └── Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='50')\n",
      "                    │       └── Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt')\n",
      "                    │           ├── Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='schedule.price')\n",
      "                    │           └── Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='10')\n",
      "                    └── Node('/keep/keep/keep/keep/Project/Selection/keep')\n",
      "                        └── Node('/keep/keep/keep/keep/Project/Selection/keep/keep')\n",
      "                            └── Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product')\n",
      "                                ├── Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='schedule')\n",
      "                                └── Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='film')\n",
      "<class 'anytree.node.node.Node'>\n",
      "[['keep']]\n",
      "[['keep']]\n",
      "[['keep']]\n",
      "[['keep']]\n",
      "[['Project']]\n",
      "[['keep'], ['Selection']]\n",
      "[['keep'], ['And'], ['keep']]\n",
      "[['keep'], ['keep'], ['Or'], ['keep']]\n",
      "[['keep'], ['eq'], ['gt'], ['lt'], ['Product']]\n",
      "[['Value', 'film.title'], ['Value', 'schedule.film_id'], ['Value', 'film.film_id'], ['Value', 'schedule.price'], ['Value', '50'], ['Value', 'schedule.price'], ['Value', '10'], ['Table', 'schedule'], ['Table', 'film']]\n"
     ]
    }
   ],
   "source": [
    "from anytree import Node, RenderTree, LevelOrderGroupIter\n",
    "# for leave in out[\"tree_list\"].leaves:\n",
    "#   print(leave)\n",
    "# from anytree.exporter import DotExporter\n",
    "# DotExporter(out[\"tree_list\"])\n",
    "print(RenderTree(out[\"tree_list\"]))\n",
    "\n",
    "print(type(out[\"tree_list\"].leaves[0]))\n",
    "\n",
    "for children in LevelOrderGroupIter(out[\"tree_list\"]):\n",
    "  level = []\n",
    "  for node in children:\n",
    "    if node.is_leaf:\n",
    "      level.append([node.name, node.val])\n",
    "    else:\n",
    "      level.append([node.name])\n",
    "  print(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2bc6710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf 0: Node('/keep/keep/keep/keep/Project/keep/keep/keep/keep/Value', val='film.title'): film.title ()\n",
      "leaf 1: Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='schedule.film_id'): schedule.film_id (Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='film.film_id'),)\n",
      "leaf 2: Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='film.film_id'): film.film_id (Node('/keep/keep/keep/keep/Project/Selection/And/keep/eq/Value', val='schedule.film_id'),)\n",
      "leaf 3: Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='schedule.price'): schedule.price (Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='50'),)\n",
      "leaf 4: Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='50'): 50 (Node('/keep/keep/keep/keep/Project/Selection/And/Or/gt/Value', val='schedule.price'),)\n",
      "leaf 5: Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='schedule.price'): schedule.price (Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='10'),)\n",
      "leaf 6: Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='10'): 10 (Node('/keep/keep/keep/keep/Project/Selection/And/Or/lt/Value', val='schedule.price'),)\n",
      "leaf 7: Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='schedule'): schedule (Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='film'),)\n",
      "leaf 8: Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='film'): film (Node('/keep/keep/keep/keep/Project/Selection/keep/keep/Product/Table', val='schedule'),)\n"
     ]
    }
   ],
   "source": [
    "for leafid, leaf in enumerate(out[\"tree_list\"].leaves):\n",
    "  print(\"leaf {}: {}: {} {}\".format(leafid, leaf, leaf.val, leaf.siblings ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31252f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chi2022] *",
   "language": "python",
   "name": "conda-env-chi2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
