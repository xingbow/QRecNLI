{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b5c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cf001f44434c44ae1f3d7be76192fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/58.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode the corpus. This might take a while\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e5d2df1e241a79856076d6b22ffad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start clustering\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 9.31 GiB (GPU 0; 11.91 GiB total capacity; 308.65 MiB already allocated; 7.51 GiB free; 326.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-470e419e4eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#min_cluster_size: Only consider cluster that have at least 25 elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_community_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clustering done after {:.2f} sec\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chi2022/lib/python3.7/site-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36mcommunity_detection\u001b[0;34m(embeddings, threshold, min_community_size, init_max_size)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# Compute cosine similarity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mcos_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# Minimum size for a community\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chi2022/lib/python3.7/site-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0ma_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mb_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 9.31 GiB (GPU 0; 11.91 GiB total capacity; 308.65 MiB already allocated; 7.51 GiB free; 326.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a more complex example on performing clustering on large scale dataset.\n",
    "This examples find in a large set of sentences local communities, i.e., groups of sentences that are highly\n",
    "similar. You can freely configure the threshold what is considered as similar. A high threshold will\n",
    "only find extremely similar sentences, a lower threshold will find more sentence that are less similar.\n",
    "A second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.\n",
    "The method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).\n",
    "In this example, we download a large set of questions from Quora and then find similar questions in this set.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "# Model for computing sentence embeddings. We use one trained for similar questions detection\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n",
    "# and find similar question in it\n",
    "url = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n",
    "dataset_path = \"quora_duplicate_questions.tsv\"\n",
    "max_corpus_size = 50000 # We limit our corpus to only the first 50k questions\n",
    "\n",
    "\n",
    "# Check if the dataset exists. If not, download and extract\n",
    "# Download dataset if needed\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(\"Download dataset\")\n",
    "    util.http_get(url, dataset_path)\n",
    "\n",
    "# Get all unique sentences from the file\n",
    "corpus_sentences = set()\n",
    "with open(dataset_path, encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in reader:\n",
    "        corpus_sentences.add(row['question1'])\n",
    "        corpus_sentences.add(row['question2'])\n",
    "        if len(corpus_sentences) >= max_corpus_size:\n",
    "            break\n",
    "\n",
    "corpus_sentences = list(corpus_sentences)\n",
    "print(\"Encode the corpus. This might take a while\")\n",
    "corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "print(corpus_embeddings.shape)\n",
    "\n",
    "print(\"Start clustering\")\n",
    "start_time = time.time()\n",
    "\n",
    "#Two parameters to tune:\n",
    "#min_cluster_size: Only consider cluster that have at least 25 elements\n",
    "#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
    "clusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)\n",
    "\n",
    "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n",
    "\n",
    "#Print for all clusters the top 3 and bottom 3 elements\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "    for sentence_id in cluster[0:3]:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])\n",
    "    print(\"\\t\", \"...\")\n",
    "    for sentence_id in cluster[-3:]:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661161fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chi2022] *",
   "language": "python",
   "name": "conda-env-chi2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
